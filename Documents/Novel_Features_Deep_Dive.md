# Novel Features Deep Dive

**Last Updated:** January 2026

Technical documentation of features that differentiate this QMS from commercial solutions. Covers implementation details, architectural decisions, and how challenges were solved.

---

## Table of Contents

1. [3D Visual Quality Management with GPU-Accelerated Heatmaps](#feature-1-3d-visual-quality-management-with-gpu-accelerated-heatmaps)
2. [AI Digital Coworker with Local LLM and Per-User RBAC](#feature-2-ai-digital-coworker-with-local-llm-and-per-user-rbac)
3. [Advanced Statistical Sampling Engine with Fallback Mechanisms](#feature-3-advanced-statistical-sampling-engine-with-fallback-mechanisms)
4. [Customer Portal with Configurable Notification Preferences](#feature-4-customer-portal-with-configurable-notification-preferences)
5. [Row-Level Security via SecureManager Pattern](#feature-5-row-level-security-via-securemanager-pattern)
6. [Hybrid Document Search with Context Windows](#feature-6-hybrid-document-search-with-context-windows)
7. [SPC Control Chart Baseline Persistence with QMS Audit Trail](#feature-7-spc-control-chart-baseline-persistence-with-qms-audit-trail)
8. [Server-Side PDF Generation via React Page Rendering](#feature-8-server-side-pdf-generation-via-react-page-rendering)
9. [Scope API - Graph-Based Data Access with Security](#feature-9-scope-api---graph-based-data-access-with-security)
10. [Handwritten Signature Capture for Compliance](#feature-10-handwritten-signature-capture-for-compliance)

**Cross-Cutting:**
- [Data Sovereignty Architecture](#innovation-1-data-sovereignty-architecture)
- [Security-First Architecture](#innovation-2-security-first-architecture-from-day-one)
- [Async-First for Non-Blocking UX](#innovation-3-async-first-for-non-blocking-ux)

[Comparison Matrix](#comparison-matrix-commercial-qms-vs-this-system)

---

## Feature 1: 3D Visual Quality Management with GPU-Accelerated Heatmaps

**Key Files:**
- `ambac-tracker-ui/src/pages/HeatMapViewer.tsx` - Heatmap visualization page
- `ambac-tracker-ui/src/components/three-d-model-viewer.tsx` - Three.js 3D viewer component
- `ambac-tracker-ui/src/pages/PartAnnotator.tsx` - Annotation interface
- `PartsTracker/Tracker/models/mes.py` - ThreeDModel, HeatmapAnnotation models

### What It Does

Upon completing a quality report, if an error type is flagged for 3D annotation an annotation viewer is displayed. This
then has the operator fill out the annotation, where it is, how severe it is, and if there's multiple instances. When
displaying the heatmaps, it shows in a gradient from red to blue on the part where the issues are, grouped and divided
across the surface of the part. This provides historical, and spatial analysis of the errors across hundreds if not
thousands of individual parts over long periods of time.

**Strategic advantage for FAI:** This same 3D annotation system can be reused for AS9102 First Article Inspection. AS9102
requires unique characteristic identification and traceability but does NOT mandate traditional 2D ballooned drawings. The
3D annotation system provides superior visualization (click on 3D model to place measurement balloons) and meets all AS9102
compliance requirements. This dramatically simplifies FAI implementation compared to traditional PDF annotation approaches.

### Why It's Novel

Most QMS systems handle defects as text entries or 2D photo uploads. This system provides interactive 3D models with
click-to-annotate defect marking and real-time GPU-accelerated heatmaps showing defect density patterns.

**Commercial Comparison:**

- **Arena PLM:** No 3D visualization; photo attachments only
- **Siemens Opcenter:** 3D CAD viewer (read-only); no defect annotation
- **MasterControl:** Text-based defect logging with photo uploads
- **ETQ Reliance:** 2D drawings with markup tools
- **Yours:** Interactive 3D models (GLB/GLTF/OBJ/STEP) with click-to-mark defects, GPU-rendered heatmaps with adjustable
  parameters (radius, intensity), automatic STEPâ†’GLB conversion

### Technical Implementation

**Architecture:**

- **Frontend:** React + Three.js for 3D rendering, WebGL context for GPU acceleration
- **Custom GLSL Shaders:** Vertex and fragment shaders for real-time heatmap rendering
- **Backend:** Django serves 3D model files from Azure Storage, ThreeDModel and HeatmapAnnotation models
- **Conversion Pipeline:** cascadio library auto-converts STEP/STP files to GLB format

**Key Technologies:**

- **Three.js (r168+):** 3D scene management, camera controls, model loading
- **WebGL / GLSL:** Custom shaders for GPU-accelerated heatmap rendering using inverse distance falloff
- **Django Storage:** Azure Blob Storage for CAD file hosting
- **File Formats:** GLB (primary), GLTF, OBJ, STEP (auto-converted)

**Code/Logic Highlights:**

```glsl
// Fragment shader - inverse distance weighting for heatmap
for (int i = 0; i < numAnnotations; i++) {
    float dist = distance(vPosition, annotations[i].position);
    float influence = intensities[i] * (radius / (dist + 0.1));
    heat += influence;
}
// Map heat value to color gradient (blue â†’ cyan â†’ green â†’ yellow â†’ red)
```

**Performance Characteristics:**

- Not formally benchmarked - performance varies by client hardware (GPU-dependent)
- System handles typical quality inspection use cases smoothly
- Browser compatibility: Modern browsers with WebGL support (Chrome, Edge, Firefox)

---

## Feature 2: AI Digital Coworker with Local LLM and Per-User RBAC

**Key Files:**
- `PartsTracker/Tracker/ai_viewsets.py` - AI chat API endpoints
- `PartsTracker/Tracker/ai_embed.py` - Document embedding and semantic search
- `PartsTracker/Tracker/models/dms.py` - DocChunk model for vector storage

### What It Does

An AI chat interface that queries both the document database and the PostgreSQL database on behalf of the user. The LLM runs locally via Ollama (air-gapped, no cloud dependency) and uses a ReAct agent pattern with five specialized tools: schema inspection, database queries, semantic document search, keyword document search, and context retrieval.

The key differentiator: the user's authentication token is forwarded through the entire chain (React â†’ LangGraph â†’ Django API), so the AI only sees data the user is authorized to access. SecureManager filtering applies to every query the AI makes.

### Why It's Novel

Most manufacturing AI is either cloud-based (data leaves premises) or uses service accounts (no per-user permissions).
This system deploys a local LLM (Ollama) with ReAct agent pattern and forwards per-user authentication tokens through
the entire chain, maintaining RBAC even in AI queries.

**Commercial Comparison:**

- **Siemens Opcenter:** Cloud-based AI features; requires internet; data sent to external servers
- **Microsoft Copilot for Dynamics 365:** Cloud-only; service account access; no RBAC enforcement in queries
- **Arena PLM:** No AI features
- **Plex AI:** Cloud-based chatbot; limited to pre-defined queries
- **Yours:** Local Ollama deployment (llama3.1:8b), on-premises inference, per-user token forwarding from React â†’
  LangGraph â†’ Django APIs, RBAC enforcement at every layer

### Technical Implementation

**Architecture:**

- **LangGraph Service:** Separate Python microservice implementing ReAct (Reasoning + Action) agent pattern
- **Ollama:** Local LLM inference engine running llama3.1:8b model
- **5 Specialized Tools:** get_schema, query_database, search_documents_semantic, search_documents_keyword, get_context
- **Token Forwarding:** User's auth token extracted from frontend request, forwarded to Django REST API in tool calls
- **Fallback:** Service account token used if user token unavailable

**Key Technologies:**

- **LangGraph:** Graph-based agent orchestration with deterministic tool execution flow
- **Ollama:** Local LLM hosting (llama3.1:8b for generation, nomic-embed-text for embeddings)
- **pgvector:** PostgreSQL extension for vector similarity search (768-dimensional embeddings)
- **Docker Compose:** ambactracker-network for service communication
- **Django REST Framework:** Token authentication with per-user permissions

**Code/Logic Highlights:**

```python
# tools.py:22 - Per-user token forwarding in LangGraph
token = config.user_token if config.user_token else config.django_api_token

headers = {"Authorization": f"Token {token}"}
response = requests.get(f"{DJANGO_API_URL}/api/parts/", headers=headers)
# Django SecureManager automatically filters by user permissions
```

**Performance Characteristics:** TBD

---

## Feature 3: Advanced Statistical Sampling Engine with Fallback Mechanisms

**Key Files:**
- `PartsTracker/Tracker/models/qms.py` - SamplingRule, SamplingRuleSet, SamplingAuditLog models
- `PartsTracker/Tracker/serializers/qms.py` - Sampling serializers

### What It Does

The sampling engine decides which parts require inspection based on configurable rules. Instead of inspecting every part (expensive) or random sampling (inconsistent), it applies rule-based logic:

- **Periodic sampling**: Inspect every Nth part (e.g., every 10th)
- **Percentage sampling**: Inspect X% of parts using deterministic hashing
- **Threshold sampling**: Increase inspection when defect rate exceeds threshold
- **Combined rules**: Layer multiple strategies with priority ordering

When defect thresholds are exceeded, the system automatically falls back to 100% inspection for a configurable duration. Every sampling decision is logged with a deterministic hash, so auditors can reproduce exactly why any given part was or wasn't inspected.

### Why It's Novel

Most QMS sampling is static ("inspect every 10th part" or "inspect 5% randomly"). This system implements a rule-based
engine with multiple sampling strategies that can be combined, plus automatic fallback to 100% inspection when defect
thresholds are exceeded.

**Commercial Comparison:**

- **MasterControl:** Fixed percentage sampling only; no dynamic adjustment
- **ETQ Reliance:** Manual sampling schedule creation; no automation
- **Arena PLM:** Basic periodic inspection (every Nth); no fallback
- **Siemens Opcenter:** Configurable sampling but no automatic escalation
- **Yours:** Rule composition (periodic + percentage + threshold + combined), priority-based rule ordering, automatic
  fallback when defects exceed threshold, configurable fallback duration, deterministic hash-based sampling for
  reproducibility

### Technical Implementation

**Architecture:**

- **SamplingRuleSet:** Container for multiple sampling rules, scoped to part type + process + step
- **SamplingRule:** Individual rule with type (PERIODIC, PERCENTAGE, THRESHOLD, COMBINED)
- **SamplingAuditLog:** Records every sampling decision with PRIMARY or FALLBACK designation
- **SamplingAnalytics:** Tracks compliance rate, defects found, effectiveness per work order
- **Deterministic Hashing:** Same inputs always produce same sampling decision (reproducibility for audits)

**Key Technologies:**

- **Django ORM:** Rule evaluation logic in Python with database-backed state
- **PostgreSQL:** Stores rule configurations, audit logs, analytics
- **Hash-based Selection:** Uses part serial number + rule parameters for consistent sampling
- **Celery (future):** Could be used for scheduled rule evaluation and alert generation

**Code/Logic Highlights:**

```python
# Deterministic sampling decision
def should_sample(part, rule):
    hash_input = f"{part.serial_number}-{rule.id}-{rule.trigger_quantity}"
    hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)

    if rule.rule_type == "PERIODIC":
        return hash_value % rule.trigger_quantity == 0
    elif rule.rule_type == "PERCENTAGE":
        return (hash_value % 100) < rule.sample_size
    # ... threshold and combined logic
```

**Performance Characteristics:** TBD

---

## Feature 4: Customer Portal with Configurable Notification Preferences

**Key Files:**
- `PartsTracker/Tracker/managers.py` - SecureManager for data isolation
- `PartsTracker/Tracker/notifications.py` - Notification system
- `PartsTracker/Tracker/tasks.py` - Celery tasks for email delivery

### What It Does

Customers access a self-service portal to track the progress of work they've commissioned. They configure their own notification preferences: which day of the week to receive updates, and frequency (weekly, biweekly, or monthly).

Notifications route through the sales email SMTP server, so customer replies go to the appropriate sales contact. On the backend, a Celery task runs every five minutes checking if any notifications are due, queues them to the task database, and sends via Django's email client with retry logic for failures.

### Why It's Novel

Most QMS portals are read-only dashboards with fixed notification schedules. This system provides self-service customer
access with granular notification preferences (which events to receive, how often, delivery channels) managed by
customers themselves.

**Commercial Comparison:**

- **Arena PLM:** Basic customer portal; read-only access; fixed email notifications
- **Siemens Opcenter:** Limited customer visibility; notifications managed by vendor
- **Plex:** Customer portal exists but notification preferences controlled by admin only
- **ETQ Reliance:** No customer portal; all communication through vendor
- **Yours:** Self-service portal with company-based data isolation, configurable notification preferences per user,
  NotificationTask model with fixed interval and deadline-based scheduling, multi-channel support (email implemented,
  in-app/SMS ready)

### Technical Implementation

**Architecture:**

- **SecureManager Filtering:** Automatic row-level filtering ensures customers only see their company's data
- **NotificationTask Model:** Database-backed scheduled notifications with status tracking (pending, sent, failed)
- **Celery Workers:** Async email delivery with retry logic
- **Notification Preferences:** Per-user configuration stored in database, evaluated before sending
- **Django Signals:** Trigger notification creation on specific events (order status changes, quality issues)

**Key Technologies:**

- **Django Authentication:** Session-based auth for browser access, token-based for API
- **Celery + Redis:** Async task queue for email delivery with exponential backoff retry
- **SMTP Integration:** Configured email backend (currently outbound-us1.ppe-hosted.com)
- **React Frontend:** Customer portal UI with TanStack Router for navigation

**Code/Logic Highlights:**

```python
# SecureManager ensures customers only see their data
class Parts(SecureModel):
    objects = SecureManager()


# In view/API
parts = Parts.objects.for_user(request.user)  # Automatic company filtering


# Notification preference check
def should_send_notification(user, event_type):
    prefs = user.notification_preferences.filter(event_type=event_type, enabled=True)
    return prefs.exists()
```

**Performance Characteristics:** TBD

---

## Feature 5: Row-Level Security via SecureManager Pattern

**Key Files:**
- `PartsTracker/Tracker/managers.py` - SecureManager, SecureQuerySet classes
- `PartsTracker/Tracker/models/core.py` - SecureModel base class
- `PartsTracker/Tracker/models/base.py` - Model inheritance structure

### What It Does

Every database query automatically filters data based on who's asking. A customer logging in only sees their own company's orders and partsâ€”not because of view-level checks that could be forgotten, but because the ORM itself enforces it.

- **Admins/Managers**: See all data across all companies
- **Operators/Employees**: See active (non-archived) records for their company
- **Customers**: See only their own company's data

The pattern uses Django's custom Manager and QuerySet classes. Every model inherits from `SecureModel`, which uses `SecureManager` as its default manager. The `for_user(user)` method applies appropriate filters based on user groups. This means security can't be accidentally bypassedâ€”even raw ORM queries go through the security layer.

### Why It's Novel

Most QMS systems implement multi-tenancy and RBAC at the application layer with hard-coded filters in views. This system
uses a custom Django ORM abstraction (SecureManager) that automatically applies security filtering at the database query
level across 30+ models.

**Commercial Comparison:**

- **Arena PLM:** Application-layer WHERE clauses; prone to bypass if developer forgets filter
- **Siemens Opcenter:** Database views per tenant; requires manual view maintenance
- **MasterControl:** Application-level checks; inconsistent enforcement
- **ETQ Reliance:** Role checks in business logic; not enforced at data layer
- **Yours:** ORM-level abstraction with `for_user()` method, automatic company filtering, soft delete integration,
  version filtering, audit log integration, SecureQuerySet base class applied to all models

### Technical Implementation

**Architecture:**

- **SecureQuerySet:** Custom QuerySet with
  methods: `for_user()`, `active()`, `deleted()`, `current_versions()`, `all_versions()`
- **SecureManager:** Custom Manager using SecureQuerySet as base, overrides default `objects` manager
- **SecureModel:** Abstract base model with SecureManager, `archived` flag, `parent_company` FK
- **Automatic Filtering:** Every query through `objects` gets security applied unless explicitly bypassed
- **Group-Based Rules:** Admin/Manager see all; Operator/Employee see active only; Customer sees own company only

**Key Technologies:**

- **Django ORM:** Custom Manager and QuerySet classes
- **Python Decorators:** Method chaining for composable filters
- **PostgreSQL:** Database-level filtering via WHERE clauses generated by ORM
- **django-auditlog:** Integrated audit logging respects same security boundaries

**Code/Logic Highlights:**

```python
class SecureQuerySet(models.QuerySet):
    def for_user(self, user):
        if user.groups.filter(name__in=['Admin', 'Manager']).exists():
            return self  # See everything
        elif user.groups.filter(name='Customer').exists():
            return self.filter(parent_company=user.parent_company)
        else:  # Operator, Employee
            return self.exclude(archived=True)


class SecureManager(models.Manager):
    def get_queryset(self):
        return SecureQuerySet(self.model, using=self._db)


class SecureModel(models.Model):
    objects = SecureManager()
    archived = models.BooleanField(default=False)
    parent_company = models.ForeignKey(Companies, ...)

    class Meta:
        abstract = True
```

**Performance Characteristics:** TBD

---

## Feature 6: Hybrid Document Search with Context Windows

**Key Files:**
- `PartsTracker/Tracker/ai_embed.py` - Embedding generation and search logic
- `PartsTracker/Tracker/models/dms.py` - Documents, DocChunk models
- `PartsTracker/Tracker/ai_viewsets.py` - Search API endpoints

### What It Does

Documents are chunked (~1200 characters, max 40 chunks per doc) and embedded using a local model (nomic-embed-text via Ollama). Search combines two approaches:

- **Semantic search**: "Find documents about heat treatment requirements" finds conceptually related content even if those exact words aren't used
- **Keyword search**: PostgreSQL full-text search for exact term matching with ranking

The LangGraph AI agent has separate tools for each search type, allowing it to choose the right approach. A context window tool retrieves surrounding chunks (Â±2) when the agent needs more context around a match.

Embedding generation happens asynchronously via Celery when a document is marked `ai_readable=True`, so uploads don't block.

### Why It's Novel

Most document systems offer either keyword search (exact matches) OR semantic search (conceptual matches), but not both.
This system combines pgvector semantic search with PostgreSQL full-text keyword search, plus a context window tool that
retrieves surrounding chunks for better LLM reasoning.

**Commercial Comparison:**

- **Confluence:** Keyword search only; no semantic understanding
- **SharePoint:** Basic keyword search with relevance ranking; no vector embeddings
- **Notion AI:** Semantic search via OpenAI API; cloud-based; no keyword fallback
- **Arena PLM:** File metadata search only; no content search
- **Yours:** Hybrid approach with pgvector (semantic) + PostgreSQL full-text (keyword) + deduplication, separate
  semantic and keyword tools for LangGraph agent, context window tool retrieves Â±2 chunks around results, async
  embedding generation via Celery signals

### Technical Implementation

**Architecture:**

- **Documents Model:** Stores file metadata with `ai_readable` flag and `classification` level
- **DocChunks Model:** ~1200 character chunks (max 40 per document) with 768-dimensional embeddings
- **pgvector Extension:** Cosine similarity search with configurable threshold (default 0.7)
- **PostgreSQL Full-Text:** ts_vector for keyword search with ranking
- **Async Embedding:** Django signal triggers Celery task when `ai_readable=True`
- **LangGraph Tools:** Separate `search_documents_semantic` and `search_documents_keyword` for granular control

**Key Technologies:**

- **pgvector:** PostgreSQL extension for vector similarity search (cosine distance)
- **Ollama nomic-embed-text:** 768-dimensional embeddings generated locally
- **PostgreSQL ts_vector:** Full-text search with ranking and stemming
- **Celery + Redis:** Async embedding generation triggered by Django signals
- **Django Signals:** post_save signal on Documents triggers embedding pipeline

**Code/Logic Highlights:**

```python
# Semantic search with pgvector
query_embedding = generate_embedding(query)
chunks = DocChunk.objects.annotate(
    similarity=CosineDistance('embedding', query_embedding)
).filter(similarity__lt=0.3).order_by('similarity')[:10]

# Keyword search with PostgreSQL full-text
chunks = DocChunk.objects.annotate(
    rank=SearchRank('search_vector', query)
).filter(search_vector=query).order_by('-rank')[:10]


# Context window retrieval
def get_context(chunk_id, window_size=2):
    chunk = DocChunk.objects.get(id=chunk_id)
    return DocChunk.objects.filter(
        doc=chunk.doc,
        chunk_index__gte=chunk.chunk_index - window_size,
        chunk_index__lte=chunk.chunk_index + window_size
    ).order_by('chunk_index')
```

**Performance Characteristics:** TBD

---

## Feature 7: SPC Control Chart Baseline Persistence with QMS Audit Trail

**Key Files:**
- `PartsTracker/Tracker/models/qms.py` - SPCBaseline model
- `PartsTracker/Tracker/viewsets/qms.py` - SPC API endpoints (freeze, supersede, active)
- `ambac-tracker-ui/src/pages/SpcPage.tsx` - SPC chart visualization
- `ambac-tracker-ui/src/pages/SpcPrintPage.tsx` - Print-optimized SPC view
- `ambac-tracker-ui/src/hooks/useSpcBaseline.ts` - React Query hooks for baseline management

### What It Does

Statistical Process Control (SPC) charts allow quality engineers to monitor process stability by plotting measurement data against calculated control limits. When a process is deemed stable (in statistical control), the control limits can be "frozen" as a baseline. All subsequent production data is then compared against these fixed limits to detect process shifts.

This system persists frozen control limits to the database with full audit trail support:
- **Who** froze the limits (user ID and name)
- **When** they were frozen (timestamp)
- **What** the limits were (UCL, CL, LCL for each chart type)
- **Why** they were superseded (reason field when unfreezing)

The SPC page supports three chart types:
- **XÌ„-R Charts:** For subgroups of 2-8 measurements (uses range for variability)
- **XÌ„-S Charts:** For subgroups of 9-25 measurements (uses standard deviation)
- **I-MR Charts:** For individual measurements (individuals and moving range)

### Why It's Novel

Most QMS systems with SPC either don't persist baselines at all (recalculate every time) or store them without proper audit trails. This implementation treats baseline management as a controlled change requiring full traceabilityâ€”critical for FDA 21 CFR Part 11 and AS9100 compliance.

**Commercial Comparison:**

- **Minitab:** Industry-standard SPC tool; saves baselines locally but no multi-user audit trail
- **InfinityQS:** Enterprise SPC; has baseline management but expensive separate licensing
- **MasterControl:** No native SPC; requires third-party integration
- **ETQ Reliance:** Basic SPC without baseline persistence
- **Arena PLM:** No SPC capabilities
- **Yours:** Integrated SPC with XÌ„-R/XÌ„-S/I-MR charts, persisted baselines with full audit trail, automatic supersession when new baseline created, baseline vs monitoring mode toggle, capability indices (Cp, Cpk, Pp, Ppk)

### Technical Implementation

**Architecture:**

- **SPCBaseline Model:** Stores frozen control limits as a point-in-time snapshot
- **Chart Type Support:** XBAR_R, XBAR_S, I_MR with appropriate limit fields
- **Status Management:** ACTIVE (one per measurement) or SUPERSEDED (historical)
- **Auto-Supersession:** Creating a new baseline automatically supersedes the previous active one
- **SecureModel Integration:** Inherits row-level security and soft-delete capabilities

**Key Technologies:**

- **Django REST Framework:** ViewSet with custom actions (`freeze`, `supersede`, `active`)
- **React Query:** Frontend hooks with cache invalidation on mutations
- **TypeScript:** Strongly-typed API client generated from OpenAPI schema
- **PostgreSQL:** DecimalField with 16,6 precision for control limit storage

**Code/Logic Highlights:**

```python
# Backend: Auto-supersession on new baseline creation
class SPCBaseline(SecureModel):
    def save(self, *args, **kwargs):
        if self.status == BaselineStatus.ACTIVE and not self.pk:
            # Supersede existing active baseline for this measurement
            SPCBaseline.objects.filter(
                measurement_definition=self.measurement_definition,
                status=BaselineStatus.ACTIVE
            ).update(
                status=BaselineStatus.SUPERSEDED,
                superseded_at=timezone.now()
            )
        super().save(*args, **kwargs)

    @property
    def control_limits(self):
        """Return limits in frontend-friendly format."""
        if self.chart_type == ChartType.I_MR:
            return {
                'individualUCL': float(self.individual_ucl),
                'individualCL': float(self.individual_cl),
                'individualLCL': float(self.individual_lcl),
                'mrUCL': float(self.mr_ucl),
                'mrCL': float(self.mr_cl),
            }
        return {
            'xBarUCL': float(self.xbar_ucl),
            'xBarCL': float(self.xbar_cl),
            'xBarLCL': float(self.xbar_lcl),
            'rangeUCL': float(self.range_ucl),
            'rangeCL': float(self.range_cl),
            'rangeLCL': float(self.range_lcl),
        }
```

```typescript
// Frontend: React Query hook with typed API
export const useSpcActiveBaseline = (measurementId: number | null) => {
    return useQuery<SpcBaselineWithLimits | null>({
        queryKey: ["spc-baseline-active", measurementId],
        queryFn: async () => {
            const response = await api.api_spc_baselines_active_retrieve({
                measurement_id: measurementId,
            });
            return (response as SpcBaselineWithLimits) || null;
        },
        enabled: measurementId !== null,
        staleTime: 5 * 60 * 1000, // Baselines don't change often
    });
};
```

**API Endpoints:**

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/api/spc-baselines/active/?measurement_id=X` | Get active baseline for measurement |
| POST | `/api/spc-baselines/freeze/` | Freeze current limits as new baseline |
| POST | `/api/spc-baselines/{id}/supersede/` | Supersede (unfreeze) with reason |
| GET | `/api/spc-baselines/` | List all baselines (filterable) |

---

## Feature 8: Server-Side PDF Generation via React Page Rendering

**Key Files:**
- `PartsTracker/Tracker/services/pdf_generator.py` - Playwright PDF generation service
- `PartsTracker/Tracker/viewsets/reports.py` - Report API endpoints
- `PartsTracker/Tracker/models/qms.py` - GeneratedReport model
- `PartsTracker/Tracker/tasks.py` - Celery task `generate_and_email_report`
- `ambac-tracker-ui/src/components/print-layout.tsx` - Shared print wrapper
- `ambac-tracker-ui/src/pages/SpcPrintPage.tsx` - Print-optimized SPC view
- `ambac-tracker-ui/src/hooks/useReportEmail.ts` - Reusable hook for any page

### What It Does

Instead of building separate PDF templates, the system renders actual React pages as PDFs using Playwright (headless Chromium). This means PDFs look exactly like what users see on screenâ€”same charts, same layouts, same data.

Flow:
1. User clicks "Email Report" on any supported page
2. API queues a Celery task
3. Playwright opens the print-optimized route (e.g., `/spc/print?processId=1&stepId=2`)
4. Waits for `[data-print-ready]` selector (ensures charts are rendered)
5. Generates PDF (Letter format, 0.5in margins)
6. Saves to Documents model (DMS integration)
7. Emails PDF to user
8. Stores `GeneratedReport` record for audit trail

### Why It's Novel

Most QMS systems use server-side PDF libraries (ReportLab, WeasyPrint) that require maintaining separate templates. When the UI changes, the PDF templates need manual updates. This approach:

- **Single source of truth**: React components render both web and PDF
- **Pixel-perfect output**: Charts, tables, styling all match the web view
- **Easy to add new reports**: Just create a print route and add config

**Commercial Comparison:**

- **MasterControl**: Server-generated PDFs with separate templates
- **ETQ Reliance**: Crystal Reports integration (separate tool)
- **Arena PLM**: Export to PDF via print dialog (client-side, inconsistent)
- **Yours**: Server-side Playwright renders actual React pages, async via Celery, DMS integration

### Technical Implementation

**Architecture:**

- **Playwright**: Headless Chromium for server-side rendering
- **Celery**: Async task processing (PDF generation can take 5-15 seconds)
- **DMS Integration**: Generated PDFs stored as Documents with audit trail
- **Email Delivery**: Django email backend sends PDF attachment

**Adding a new report type** requires only:
1. Add config to `PdfGenerator.REPORT_CONFIG`
2. Create print page component with `[data-print-ready]` attribute
3. Add route and button with `useReportEmail()` hook

**Performance Characteristics:** TBD

---

## Feature 9: Scope API - Graph-Based Data Access with Security

**Key Files:**
- `PartsTracker/Tracker/scope.py` - Core graph traversal functions
- `PartsTracker/Tracker/models/core.py` - SecureModel base class integration

### What It Does

The Scope API provides graph traversal utilities for navigating the model hierarchy. Given any object (an Order, a Part, a Work Order), it can:

- **get_descendants(obj)**: Find all objects below it (Parts â†’ WorkOrders â†’ QualityReports â†’ Annotations)
- **get_ancestors(obj)**: Find all objects above it (trace back to the Order)
- **related_to(Model, obj)**: Get all Documents/Annotations/Approvals attached anywhere in that subtree
- **explain_path(from, to)**: Describe the relationship path between two objects

The key insight: every query respects the user's permissions. If you call `get_descendants(order, user=request.user)`, security filtering happens at every hop through the graph. A customer can only traverse into objects their company owns.

This powers features like "show all documents related to this order" without writing complex joinsâ€”one function call traverses the entire hierarchy with security baked in.

### Why It's Novel

Most systems handle hierarchical data with either:
- Hard-coded joins (fragile, miss edge cases)
- Raw recursive queries (no security, performance issues)
- Separate APIs per relationship (inconsistent, duplicated logic)

This system uses a single graph traversal algorithm with batched queries, depth limiting, type filtering, and automatic security enforcement via `for_user()` at every level.

**Commercial Comparison:**

- **Arena PLM**: Fixed parent-child queries; no arbitrary graph traversal
- **Siemens Opcenter**: SQL views per relationship; manual maintenance
- **MasterControl**: Application-level joins; inconsistent security enforcement
- **Yours**: Generic graph traversal with `get_descendants()`, `get_ancestors()`, `related_to()`, `explain_path()`, batched queries for performance, depth/type filtering, automatic SecureManager integration at every hop

### Technical Implementation

**Architecture:**

- **BFS Traversal**: Breadth-first search through Django model relationships
- **Batched Queries**: Collects all objects at each depth level, fetches in single queries per model type
- **ContentType Tracking**: Returns `{content_type_id: set(object_ids)}` for efficient filtering
- **Security Integration**: Passes `user` parameter through traversal, calls `for_user()` on every queryset
- **Generic Relation Support**: `related_to()` builds Q filters for GenericForeignKey attachments

**Key Technologies:**

- **Django ORM**: Custom QuerySet traversal via `_meta.get_fields()`
- **ContentType Framework**: Maps model classes to IDs for generic relations
- **ForeignObjectRel**: Detects reverse relationships for downward traversal

**Code/Logic Highlights:**

```python
# Get all objects below an order, filtered by user permissions
from Tracker.scope import get_descendants, related_to

objects_by_type = get_descendants(order, user=request.user)
# Returns: {content_type_id: {obj_id, obj_id, ...}, ...}

# Get all documents attached anywhere in the order's subtree
docs = related_to(Documents, order, user=request.user)
# Single call traverses: Order â†’ Parts â†’ WorkOrders â†’ QualityReports
# Returns only documents the user has permission to see

# Explain how two objects are related
path = explain_path(order, quality_report)
# Returns: [(order, 'parts'), (part, 'workorder_set'),
#           (workorder, 'qualityreports_set'), (report, None)]
```

**Performance Characteristics:** TBD

---

## Feature 10: Handwritten Signature Capture for Compliance

**Key Files:**
- `ambac-tracker-ui/src/components/approval/SignatureCanvas.tsx` - Canvas drawing component
- `ambac-tracker-ui/src/components/approval/SignatureVerification.tsx` - Combined signature + password verification
- `PartsTracker/Tracker/models/qms.py` - CAPATasks.completion_signature field
- `PartsTracker/Tracker/migrations/0015_add_signature_fields_to_capa_tasks.py` - Schema migration

### What It Does

For compliance-critical actions (CAPA task completion, document approvals), the system captures actual handwritten signatures. Users draw their signature with mouse or finger (touch devices), then re-enter their password to verify identity.

The flow:
1. User draws signature on canvas (touch-enabled, responsive sizing)
2. System converts to base64 PNG (`toDataURL("image/png")`)
3. User checks confirmation box ("I confirm this is my signature...")
4. User enters password for identity verification
5. Backend validates password before accepting the action
6. Signature stored as base64 text in database, linked to the record

This satisfies 21 CFR Part 11 requirements for electronic signatures in FDA-regulated environments: the signature is tied to a specific person (password verification), captured at a specific time (audit trail), and visually represents intent (handwritten).

### Why It's Novel

Most QMS systems use either:
- Simple "I agree" checkboxes (not a true signature)
- Typed name fields (no visual signature)
- Third-party e-signature services (data leaves premises, subscription costs)

This implementation provides real handwritten signatures captured in-browser, stored on-premises, with password verificationâ€”without external dependencies.

**Commercial Comparison:**

- **DocuSign/Adobe Sign**: Cloud-based, subscription model, data leaves premises
- **MasterControl**: Typed signatures with password; no handwritten capture
- **ETQ Reliance**: Checkbox-based acknowledgment; not FDA Part 11 compliant for signatures
- **Arena PLM**: Electronic approval with password; no signature capture
- **Yours**: In-browser canvas drawing, base64 PNG storage, password re-verification, touch device support, fully on-premises

### Technical Implementation

**Architecture:**

- **SignatureCanvas**: React component wrapping react-signature-canvas library
- **SignatureVerification**: Composite component bundling canvas + confirmation + password
- **High-DPI Support**: Canvas scales by devicePixelRatio for crisp rendering on Retina displays
- **Responsive Sizing**: Canvas resizes with container, preserves signature data across resize
- **Storage**: Base64 PNG stored in TextField (PostgreSQL), typically 10-50KB per signature

**Key Technologies:**

- **react-signature-canvas**: Canvas-based signature capture with touch support
- **Canvas API**: `toDataURL()` for PNG export, `fromDataURL()` for restoration
- **Device Pixel Ratio**: Handles high-DPI displays for crisp signatures

**Code/Logic Highlights:**

```typescript
// SignatureCanvas handles drawing and export
const handleEnd = () => {
    if (sigPadRef.current && !sigPadRef.current.isEmpty()) {
        const dataUrl = sigPadRef.current.toDataURL("image/png");
        onChange(dataUrl);  // Base64 PNG: "data:image/png;base64,..."
    }
};

// SignatureVerification bundles the full compliance flow
export interface SignatureVerificationData {
    signature_data: string;  // Base64 PNG
    password: string;        // For re-authentication
    confirmed: boolean;      // "I confirm this is my signature"
}
```

```python
# Backend: Signature stored with completion record
class CAPATasks(SecureModel):
    completion_signature = models.TextField(
        blank=True, null=True,
        help_text='Base64-encoded signature image data'
    )
    requires_signature = models.BooleanField(
        default=False,
        help_text='If true, task completion requires signature and password verification'
    )
```

**Performance Characteristics:** TBD

---

## Cross-Cutting Innovations

### Innovation 1: Data Sovereignty Architecture

**Pattern:** All sensitive data and AI processing stays on-premises; zero cloud dependencies for manufacturing data.

**Where It's Used:**

- Local LLM deployment (Ollama instead of OpenAI/Anthropic)
- On-premises vector embeddings (nomic-embed-text via Ollama)
- Self-hosted PostgreSQL with pgvector (no managed vector DB services)
- Azure Storage (customer-controlled) vs. vendor SaaS storage

**Why It Matters:**

- **ITAR Compliance:** Technical data cannot leave US-controlled systems
- **CUI Protection:** Controlled Unclassified Information requires data sovereignty
- **Competitive Advantage:** Defense contractors can't use cloud AI solutions; this can
- **Cost Structure:** No per-query API costs; fixed infrastructure costs only

### Innovation 2: Security-First Architecture from Day One

**Pattern:** Security and compliance built into foundation, not added later.

**Where It's Used:**

- SecureManager on all models from initial migration
- Document classification system from first file upload
- Audit logging (django-auditlog) on all model changes from start
- RBAC groups and permissions designed before feature development
- Token-based authentication with per-user forwarding through services

**Why It Matters:**

- **No Refactoring:** Avoids costly security retrofits that plague most projects
- **Compliance-Ready:** 80% NIST 800-171 compliant from architecture, not bolt-ons
- **Reduced Risk:** Security bugs prevented by design, not caught in review
- **Faster Development:** Security abstraction (SecureManager) makes secure code easier to write

### Innovation 3: Async-First for Non-Blocking UX

**Pattern:** All heavy processing moved to Celery workers; web requests never block.

**Where It's Used:**

- Document embedding generation (triggered by signal, runs async)
- HubSpot CRM sync (hourly Celery Beat task)
- Email notifications (queued via Celery, retried on failure)
- Future: Report generation, data exports, backup operations

**Why It Matters:**

- **Responsive UI:** Users never wait for slow operations
- **Reliability:** Retry logic handles transient failures automatically
- **Scalability:** Worker pool scales independently from web tier
- **Observability:** Celery result backend tracks task status for debugging

---

## Comparison Matrix: Commercial QMS vs. This System

| Feature Category         | Arena PLM    | Siemens Opcenter | MasterControl | ETQ Reliance | This System                 |
|--------------------------|--------------|------------------|---------------|--------------|-----------------------------|
| **3D Visualization**     | âŒ None       | ğŸ”¶ Viewer only   | âŒ None        | âŒ None       | âœ… Interactive + heatmaps    |
| **AI/LLM Integration**   | âŒ None       | ğŸ”¶ Cloud only    | âŒ None        | âŒ None       | âœ… Local LLM + RBAC          |
| **Statistical Sampling** | ğŸ”¶ Fixed %   | ğŸ”¶ Basic rules   | ğŸ”¶ Manual     | ğŸ”¶ Basic     | âœ… Dynamic + fallback        |
| **SPC with Baselines**   | âŒ None       | ğŸ”¶ Separate tool | âŒ Integration | ğŸ”¶ Basic     | âœ… Integrated + audit trail  |
| **Customer Portal**      | ğŸ”¶ Read-only | ğŸ”¶ Basic         | âŒ None        | ğŸ”¶ Limited   | âœ… Self-service + prefs      |
| **RBAC Enforcement**     | âœ… App-level  | âœ… App-level      | âœ… App-level   | âœ… App-level  | âœ… ORM-level (SecureManager) |
| **Document Search**      | ğŸ”¶ Keyword   | ğŸ”¶ Keyword       | ğŸ”¶ Keyword    | ğŸ”¶ Basic     | âœ… Hybrid semantic + keyword |
| **PDF Generation**       | ğŸ”¶ Print dialog | ğŸ”¶ Separate tool | ğŸ”¶ Templates | ğŸ”¶ Crystal Reports | âœ… Reactâ†’PDF via Playwright |
| **Graph Data Access**    | âŒ Fixed joins | ğŸ”¶ SQL views    | âŒ Manual     | âŒ Manual    | âœ… Scope API with security   |
| **E-Signatures**         | ğŸ”¶ Password only | ğŸ”¶ Password only | ğŸ”¶ Typed name | ğŸ”¶ Checkbox | âœ… Handwritten + password    |

**Legend:**

- âœ… Full implementation
- ğŸ”¶ Partial/basic implementation
- âŒ Not available
